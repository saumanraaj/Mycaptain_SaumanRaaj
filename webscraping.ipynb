{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "webscraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1U88mVmaM1ZOPHKjnEMKx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saumanraaj/Mycaptain_SaumanRaaj/blob/main/webscraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ecKsyP41n2m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "565efbac-d49c-441f-d042-cac9a471908e"
      },
      "source": [
        "  # web scraping using beautifulsoup and requests\n",
        "  import pandas\n",
        "  import requests\n",
        "  import databases \n",
        "  import argparse\n",
        "\n",
        "  parser=argparse.ArgumentParser()\n",
        "  parser.add_argument(\"--page_num_Max\", help=\"enter the number of pages to parse\", type=int)\n",
        "  parser.add_argument(\"--dbname\",help=\"enter the pages toparse\",type=int)\n",
        "  args=parser.parse_args()\n",
        "  from bs4 import BeautifulSoup\n",
        "  oyo_url=\"https://www.oyorooms.com/hotels-in-bangalore/?page=\"\n",
        "  page_num_max= args.page_num_Max\n",
        "  databases.connect(args.dbname)\n",
        "  for page_num in range(1,page_num_max):\n",
        "    req= requests.get(oyo_url+ str(page_num)) #getting the information from this url\n",
        "    content= req.content #accecsing req object from class requests to get the content of the url\n",
        "    print(content)\n",
        "    scraped_info_list=[]\n",
        "\n",
        "  \n",
        "\n",
        "    soup= BeautifulSoup(content,\"html.parser\") #creating an object called soup from beautifulsoup cls to parse the html content\n",
        "    all_hotels= soup.find_all(\"div\",{\"class\":\"hotelCardListing\"}) #finding all the html code with tag div attribute class and name hotelcardlisting so that u can get the general strucuture for all hotel data list\n",
        "    #all_hotels is basically a list which returns value which satisfy soup's findall condition\n",
        "\n",
        "    for hotels in all_hotels:\n",
        "      hotel_dict={} #creating a dictionary to store all parsed values from the website\n",
        "      hotel_dict[\"name\"]= hotel.find(\"h3\",{\"class\":\"listingHotelDescription__hotelName\"}).txt #getting the name of hotels from the all_hotel's list\n",
        "      hotel_dict[\"address\"]=hotel.find(\"span\",{\"itemprop\":\"streetAdress\"}).txt #getting the address for hotels \n",
        "      hotel_dict[\"price\"]=hotel.find(\"span\",{\"class\":\"listingPrice__finalPrice\"}).txt #getting hotel prices\n",
        "      #try except block: here there might be new hotels without ratings so when we parse them we get error 'AttributeError' to avoid that we ask the compiler to exceute inspite of it using try and except\n",
        "      try:\n",
        "        hotel_dict[\"rating\"]=hotel.find(\"span\",{\"class\":\"hotelRating__ratingSummary\"}).txt #getting hotel reviews\n",
        "      except AttributeError: #this try while excecuting throws the error 'AttributError' as the hotel rating is not present but the except block catches it and pass makes the code to un inspite of the error\n",
        "        hotel_dict[\"raing\"]= None\n",
        "      amnity_list=[]\n",
        "      parent_amnities= hotel.find(\"div\",{\"class\":\"amnityWrapper\"}) #getting the overall amnities offered by the hotel\n",
        "      for amnity in parent_amnities.find_all(\"dive\",{\"class\":\"amnityWrapper__amnity\"}): #stripping down individual amnities and storing them in a list \n",
        "        amnity_list.append(amnity.find(\"span\",{\"class\":\"d-body-sm d-textEllipsis\"}).text.strip())\n",
        "      hotel_dict[\"amnities\"]=', '.join(amnity_list[:-1]) #takes the list'amnity_list' and join it's elements using ,  in between them #here u are removing the unwnated last element(junk) from ur amnities list\n",
        "      \n",
        "      scraped_info_list.append(hotel_dict) #storing the dictionary as list to help store info as csv file\n",
        "      databases.insert_into_table(args.dbname, tuple(hotel_dict.values()))#storing values in a table in sql database\n",
        "\n",
        "    #dataframe= pandas.DataFrame(scraped_info_list) this data frame is actually a data structure used by pandas to store data\n",
        "    #dataframe.to_csv(\"oyo.csv\")converting data to csv file named oyo\n",
        "    databases.get_hotel_info(args.dbname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3b00e8ee44e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatabases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'databases'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}